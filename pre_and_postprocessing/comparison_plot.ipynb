{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "421467bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dmr_data(filepath):\n",
    "    init_data = []\n",
    "    last_fini = None\n",
    "\n",
    "    with open(filepath, \"r\") as file:\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "\n",
    "            if stripped.startswith(\"DMR INIT\"):\n",
    "                timestamp_match = re.search(r\"Timestamp:\\s*(\\d+\\.\\d+)\", stripped)\n",
    "                nodes_match = re.search(r\"Nodes:\\s*(\\d+)\", stripped)\n",
    "                if timestamp_match and nodes_match:\n",
    "                    timestamp = float(timestamp_match.group(1))\n",
    "                    nodes = int(nodes_match.group(1))\n",
    "                    init_data.append((timestamp, nodes))\n",
    "\n",
    "            elif stripped.startswith(\"DMR FINI\"):\n",
    "                timestamp_match = re.search(r\"Timestamp:\\s*(\\d+\\.\\d+)\", stripped)\n",
    "                nodes_match = re.search(r\"Nodes:\\s*(\\d+)\", stripped)\n",
    "                if timestamp_match and nodes_match:\n",
    "                    last_fini = (\n",
    "                        float(timestamp_match.group(1)),\n",
    "                        int(nodes_match.group(1)),\n",
    "                    )\n",
    "\n",
    "    if last_fini:\n",
    "        init_data.append(last_fini)\n",
    "\n",
    "    return init_data\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "data1 = load_dmr_data(\"two-dmr-exec-newest/output_668.txt\")\n",
    "data2 = load_dmr_data(\"two-dmr-exec-newest/output_670.txt\")\n",
    "\n",
    "# Flatten to find the earliest start time\n",
    "all_timestamps = [t for t, _ in data1 + data2]\n",
    "if not all_timestamps:\n",
    "    raise ValueError(\"No data found in either file.\")\n",
    "\n",
    "global_start_time = min(all_timestamps)\n",
    "\n",
    "\n",
    "# Normalize to global start time\n",
    "def normalize_by_global_start(data, global_start):\n",
    "    return zip(*[(t - global_start, n) for t, n in data])\n",
    "\n",
    "\n",
    "times1, nodes1 = normalize_by_global_start(data1, global_start_time)\n",
    "times2, nodes2 = normalize_by_global_start(data2, global_start_time)\n",
    "\n",
    "mall_times1 = [times1[0], *times1, times1[-1]]\n",
    "mall_nodes1 = [0, *nodes1, 0]\n",
    "mall_times2 = [times2[0], *times2, times2[-1]]\n",
    "mall_nodes2 = [0, *nodes2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f94530f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "def get_slurm_df(file: Path) -> pl.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            fields = dict(re.findall(r\"(\\S+?)=(\\S+)\", line))\n",
    "            rows.append(fields)\n",
    "\n",
    "    df = pl.DataFrame(rows)\n",
    "    fine_df = df.select(\n",
    "        pl.col(\"JobId\"), pl.col(\"JobState\"), pl.col(\"StartTime\"), pl.col(\"EndTime\")\n",
    "    ).filter(pl.col(\"JobState\") == \"COMPLETED\")\n",
    "\n",
    "    return fine_df\n",
    "\n",
    "\n",
    "def find_jobcomp(jobid: str, jc_df: pl.DataFrame) -> tuple[str, str, str, str]:\n",
    "    row = jc_df.row(by_predicate=pl.col(\"JobId\") == jobid)\n",
    "    return row\n",
    "\n",
    "\n",
    "def get_timestamps(row: tuple[str, str, str, str]) -> tuple[float, float]:\n",
    "    _, _, start, end = row\n",
    "    start_ts = datetime.fromisoformat(start).timestamp()\n",
    "    end_ts = datetime.fromisoformat(end).timestamp()\n",
    "    return start_ts, end_ts\n",
    "\n",
    "\n",
    "slurm_jobcomp_path = Path().resolve() / \"slurm_jobcomp.log\"\n",
    "\n",
    "jobid1 = \"976\"\n",
    "jobid2 = \"977\"\n",
    "\n",
    "jc_df = get_slurm_df(file=slurm_jobcomp_path)\n",
    "\n",
    "workload1 = find_jobcomp(jobid=jobid1, jc_df=jc_df)\n",
    "workload2 = find_jobcomp(jobid=jobid2, jc_df=jc_df)\n",
    "\n",
    "real_ts1 = get_timestamps(workload1)\n",
    "real_ts2 = get_timestamps(workload2)\n",
    "global_start_time = min(real_ts1[0], real_ts2[0])\n",
    "normalized_ts1 = list(i - global_start_time for i in real_ts1)\n",
    "normalized_ts2 = list(i - global_start_time for i in real_ts2)\n",
    "baseline_ts1 = normalized_ts1[:1] + normalized_ts1\n",
    "baseline_ts2 = normalized_ts2[:1] + normalized_ts2\n",
    "\n",
    "baseline_nodes = [0, 3, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ffee903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_workflow_log(file: Path) -> list[tuple[str, str]]:\n",
    "    with open(file, \"r\") as fp:\n",
    "        data = fp.read()\n",
    "\n",
    "    pattern = r\"Scheduled job /loop/([^/]+)/\\d+\\.\\d+ with job id (\\d+)\"\n",
    "    res = [(i.group(1), i.group(2)) for i in re.finditer(pattern, data)]\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_timed_node_usage_from_jobcomp(\n",
    "    file: Path, jc_df: pl.DataFrame\n",
    ") -> tuple[list[float], list[int]]:\n",
    "    steps = walk_workflow_log(file)\n",
    "\n",
    "    exe_to_coeff = {\"clustering\": 3, \"silhouette\": 1, \"annealing\": 0}\n",
    "\n",
    "    times = []\n",
    "    nodes = []\n",
    "\n",
    "    for step in steps:\n",
    "        name, jobid = step\n",
    "        coeff = exe_to_coeff[name]\n",
    "\n",
    "        job_row = jc_df.row(by_predicate=pl.col(\"JobId\") == jobid)\n",
    "        _, _, start, end = job_row\n",
    "        start_ts = datetime.fromisoformat(start).timestamp()\n",
    "        end_ts = datetime.fromisoformat(end).timestamp()\n",
    "\n",
    "        times.extend([start_ts, end_ts])\n",
    "        nodes.extend([coeff, 0])\n",
    "\n",
    "    return times, nodes\n",
    "\n",
    "\n",
    "perf_folder = Path().resolve().parent / \"perf\"\n",
    "\n",
    "wf_times1_raw, wf_nodes1 = get_timed_node_usage_from_jobcomp(\n",
    "    file=perf_folder / \"workflow_sleep_duo1_1\" / \"log.txt\", jc_df=jc_df\n",
    ")\n",
    "wf_times2_raw, wf_nodes2 = get_timed_node_usage_from_jobcomp(\n",
    "    file=perf_folder / \"workflow_sleep_duo1_2\" / \"log.txt\", jc_df=jc_df\n",
    ")\n",
    "\n",
    "wf_global_start = min(wf_times1_raw + wf_times2_raw)\n",
    "wf_times1 = [i - wf_global_start for i in wf_times1_raw]\n",
    "wf_times2 = [i - wf_global_start for i in wf_times2_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_and_area(ax, x, y, label: str):\n",
    "    linestyle = \"solid\"\n",
    "    ax.fill_between([x[0], *x, x[-1]], [0, *y, 0], step=\"post\", alpha=0.4)\n",
    "    ax.step(\n",
    "        [x[0], *x, x[-1]],\n",
    "        [0, *y, 0],\n",
    "        label=label,\n",
    "        linewidth=2,\n",
    "        linestyle=linestyle,\n",
    "        where=\"post\",\n",
    "    )\n",
    "\n",
    "# Plot\n",
    "scaling = 0.85\n",
    "fig, axs = plt.subplots(3, sharex=True, figsize=(6.4 * scaling, 4.8 * scaling))\n",
    "\n",
    "linestyle = \"solid\"\n",
    "\n",
    "axs[0].title.set_text(\"Baseline compute nodes usage\")\n",
    "step_and_area(axs[0], baseline_ts1, baseline_nodes, label=\"Job 1\")\n",
    "step_and_area(axs[0], baseline_ts2, baseline_nodes, label=\"Job 2\")\n",
    "\n",
    "axs[1].title.set_text(\"Workflow compute nodes usage\")\n",
    "step_and_area(axs[1], wf_times1, wf_nodes1, label=\"Job 1\")\n",
    "step_and_area(axs[1], wf_times2, wf_nodes2, label=\"Job 2\")\n",
    "\n",
    "axs[2].title.set_text(\"Malleability compute nodes usage\")\n",
    "step_and_area(axs[2], mall_times1, mall_nodes1, label=\"Job 1\")\n",
    "step_and_area(axs[2], mall_times2, mall_nodes2, label=\"Job 2\")\n",
    "\n",
    "axs[0].set_yticks(np.arange(0, 3 + 1, 1))\n",
    "axs[1].set_yticks(np.arange(0, 3 + 1, 1))\n",
    "axs[2].set_yticks(np.arange(0, 3 + 1, 1))\n",
    "# axs[0].grid()\n",
    "# axs[1].grid()\n",
    "# axs[2].grid()\n",
    "\n",
    "\n",
    "fig.supxlabel(\"Time since earliest job start (s)\")\n",
    "fig.supylabel(\"Number of allocated compute nodes\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.show()\n",
    "fig.savefig(\"comparison_plot.pdf\", format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
